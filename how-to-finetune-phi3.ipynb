{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Prerequisites\n",
    "\n",
    "Before starting, ensure your Azure Services are operational, your Conda environment is configured, and your environment variables are set as described in the [README.md](README.md) document.\n",
    "\n",
    "## üìã Table of Contents\n",
    "\n",
    "This notebook provides a practical guide to enhancing the relevance of Phi-3 models through fine-tuning with Azure Machine Learning:\n",
    "\n",
    "1. [**Introduction to Fine-Tuning**](#define-field-types): Delve into the essentials of fine-tuning and Retrieval Augmented Generation (RAG) for Phi-3 models. This section covers their importance, benefits, and the strategic approach to customizing language models, offering insights into their technical aspects and real-world applications.\n",
    "2. [**Exploring the Phi-3 Model Universe**](#configuring-vector-search): Exploring the expanded Phi-3 model family\n",
    "3. [**Use Case: Enhancing Query Retrieval with Phi-3 Fine-Tuning**](#configuring-semantic-search): Learn how fine-tuned SLM models can drastically enhance search capabilities, resulting in a more efficient and accurate retrieval system.\n",
    "\n",
    "For additional information, refer to the following resources:\n",
    "- [Phi-3 Release Documentation](https://azure.microsoft.com/en-us/blog/introducing-phi-3-redefining-whats-possible-with-slms/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory changed to C:\\Users\\pablosal\\Desktop\\gbba-ai-small-language-models\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the target directory\n",
    "target_directory = r\"C:\\Users\\pablosal\\Desktop\\gbba-ai-small-language-models\"  # change your directory here\n",
    "\n",
    "# Check if the directory exists\n",
    "if os.path.exists(target_directory):\n",
    "    # Change the current working directory\n",
    "    os.chdir(target_directory)\n",
    "    print(f\"Directory changed to {os.getcwd()}\")\n",
    "else:\n",
    "    print(f\"Directory {target_directory} does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to Fine-Tuning Phi-3 models\n",
    "\n",
    "In the dynamic world of language models, fine-tuning and Retrieval Augmented Generation (RAG) are key to unlocking superior model performance. This guide explores their roles, benefits, and the strategic hierarchy of language model customization, shedding light on their technical nuances and practical applications.\n",
    "\n",
    "#### What is Fine-Tuning?\n",
    "\n",
    "Fine-tuning refines a pre-trained language model (LLM) by training it further on specific tasks or datasets. This process aims to boost the model's accuracy and efficiency by optimizing its parameters for the task at hand.\n",
    "\n",
    "#### Benefits of Fine-Tuning\n",
    "\n",
    "- **üöÄ Enhanced Performance:** Tailoring models to specific tasks significantly improves accuracy and efficiency.\n",
    "- **üõë Reduced Hallucinations:** Fine-tuning helps minimize the generation of irrelevant or fabricated content.\n",
    "- **üí∏ Cost-Effectiveness:** Enables smaller models to achieve remarkable task-specific performance, avoiding the need for larger, costlier models.\n",
    "- **üé® Customization:** Facilitates the creation of models uniquely adapted to specific requirements or datasets.\n",
    "\n",
    "### Technical Approach: Azure OpenAI's Strategy\n",
    "\n",
    "Azure OpenAI employs low-rank approximation (LoRA) for efficient fine-tuning, focusing on adjusting a subset of key parameters to streamline the fine-tuning process while maintaining model performance.\n",
    "\n",
    "#### üìö Hierarchy of Language Model Customization\n",
    "\n",
    "1. **Prompt Engineering:** A simple yet effective method for guiding model responses with well-crafted prompts.\n",
    "2. **Retrieval Augmented Generation (RAG):** Augments prompts with external data for up-to-date information, without modifying the model.\n",
    "3. **Fine-Tuning:** Provides deeper customization for specific tasks, though it's used less frequently due to its complexity.\n",
    "4. **Training a New Model:** The most resource-intensive method, involving the development of a model from the ground up.\n",
    "\n",
    "#### üé® Types of Fine-Tuning\n",
    "\n",
    "- **Continual Pre-Training (CPT):** Further pre-training with additional data, albeit costly and with limited applicability.\n",
    "- **Supervised Fine-Tuning (SFT):** Adapts models to new tasks with prompt-completion pairs, requiring high-quality data.\n",
    "- **Reinforcement Learning from Human Feedback (RLHF):** Aligns models with human preferences through feedback, offering high steerability at the expense of quality data.\n",
    "\n",
    "#### Fine-Tuning Options: Hosted vs. Self-Managed Options on Azure\n",
    "\n",
    "When it comes to fine-tuning the Phi-3 model, Azure offers two distinct pathways tailored to different operational needs and expertise levels: **Cloud-Hosted Managed Services** and **Self-Managed Fine-Tuning**. Each option provides unique advantages, allowing you to choose the best fit for your project's requirements.\n",
    "\n",
    "### üåê Cloud-Hosted Managed Services: Streamline Your AI Operations\n",
    "\n",
    "Azure's Cloud-Hosted Managed Services offer a hassle-free way to fine-tune your models, ideal for teams looking to streamline their AI operations without the overhead of managing infrastructure. This option is perfect for:\n",
    "\n",
    "- **Fully Managed Services:** Azure takes care of all the underlying infrastructure, allowing you to focus solely on fine-tuning your models.\n",
    "- **Scalability:** Easily scale your operations up or down based on your needs, without worrying about hardware limitations.\n",
    "- **Security and Compliance:** Benefit from Azure's built-in security features, ensuring your data and models are protected according to the highest standards.\n",
    "\n",
    "Ex. UI out-of-the-box fine-tuning of Phi-3 in Azure AI Studio: https://github.com/microsoft/Phi-3CookBook/blob/main/md/04.Fine-tuning/FineTuning_AIStudio.md\n",
    "\n",
    "### üõ†Ô∏è Self-Managed Fine-Tuning: Harnessing Open-Source Tools\n",
    "\n",
    "For teams with the capability to manage their infrastructure and who prefer a hands-on approach, Self-Managed Fine-Tuning on Azure provides the flexibility to use open-source tools and frameworks. This option is well-suited for:\n",
    "\n",
    "- **\"White Box\" LLMs Fine-Tuning:** Efficiently fine-tune open-source or \"white box\" Large Language Models on small GPU clusters or even single GPU machines, offering a cost-effective solution for smaller projects.\n",
    "- **Advanced Techniques:** Leverage the full power of Azure's infrastructure to experiment with advanced fine-tuning techniques, such as reinforcement learning, to push the boundaries of what your models can achieve.\n",
    "- **Customization and Control:** Enjoy complete control over your fine-tuning environment, allowing for deep customization and optimization of your models and infrastructure.\n",
    "\n",
    "We will showcase this path using Azure ML, learning to manage our infrastructure remotely (cloud infrastructure) and leveraging the library peft to utilize QLoRA for a fine-tuning approach.\n",
    "\n",
    "### Choosing Your Path\n",
    "\n",
    "Whether you opt for the ease and simplicity of Cloud-Hosted Managed Services or the flexibility and control of Self-Managed Fine-Tuning, Azure provides a robust platform to support your Phi-3 fine-tuning endeavors. Consider your team's expertise, project requirements, and operational capacity when choosing the best path forward.\n",
    "\n",
    "### ‚ö†Ô∏è Cautions and Best Practices\n",
    "\n",
    "#### Cautions\n",
    "\n",
    "- **Time and Cost:** Fine-tuning requires significant investment, making it less suitable for projects needing quick turnarounds.\n",
    "- **Data Volatility:** Rapid changes in datasets can challenge the fine-tuning process.\n",
    "- **Explainability and Grounding:** Initial methods might prioritize alternatives to fine-tuning when these aspects are crucial.\n",
    "\n",
    "#### Best Practices\n",
    "\n",
    "- **Data Quality:** Emphasize the collection of large, high-quality datasets for fine-tuning.\n",
    "- **Baseline Performance:** Utilize prompt engineering or RAG to set performance baselines and pinpoint improvement areas.\n",
    "- **Iterative Evaluation:** Apply both quantitative and qualitative measures to continually enhance model performance.\n",
    "\n",
    "#### ü§ù Integrating Fine-Tuning with RAG\n",
    "\n",
    "Fine-tuning and RAG complement each other, with fine-tuning modifying the model and RAG altering the input data. Their synergy can significantly boost model performance for specific tasks, as demonstrated by case studies from Microsoft Research and other practical applications. https://arxiv.org/pdf/2401.08406\n",
    "\n",
    "### üÜö Comparing Small and Large Language Models\n",
    "\n",
    "What sets Small Language Models (SLMs) apart from Large Language Models (LLMs)? \n",
    "\n",
    "The core difference between SLMs and LLMs lies in their inherent capabilities and the scale of data they're trained on. LLMs excel in tasks requiring extensive world knowledge, complex reasoning, and advanced functionalities like orchestration and tool use. They're ideal for open-ended applications where the breadth of understanding and creativity is paramount. On the other hand, SLMs shine in more narrowly defined scenarios, where the focus is on specific language tasks or reasoning within a constrained domain. \n",
    "\n",
    "The choice between using an SLM or an LLM isn't binary but rather depends on the specific requirements of your project. Our experience in developing various AI-driven tools has shown that both types of models have their unique advantages and can be complementary. Here are some considerations to help you decide:\n",
    "\n",
    "### When to Opt for SLMs:\n",
    "\n",
    "- **Resource Efficiency:** If computational resources are limited, SLMs are the way to go. They require less power, making them more accessible for projects with budget constraints.\n",
    "- **Faster Inference:** The compact size of SLMs allows for quicker data processing, enabling real-time applications even on local machines.\n",
    "- **Domain-Specific Tasks:** For projects focused on a particular niche, SLMs can be more practical. They're not only quicker but also more cost-effective to fine-tune, allowing for tailored solutions without the hefty price tag.\n",
    "\n",
    "Fine-tuning smaller models, such as phi-3, presents a strategic alternative to leveraging larger models for certain applications. With their reduced demand for computational power and memory, SLMs offer a practical solution for tasks that don't require the full might of an LLM. This approach not only accelerates training times but also minimizes operational costs, making SLMs a compelling choice for a wide range of applications.\n",
    "\n",
    "In this guide, we'll take you through a comprehensive journey of fine-tuning phi-3, demonstrating how to harness its capabilities for domain-specific challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Phi-3 Model Universe\n",
    "\n",
    "On the remarkable date of May 21, 2024, Microsoft unveiled a groundbreaking expansion to their Phi-3 model family. Below is a comparison of the new additions:\n",
    "\n",
    "| Model         | Parameters | Context Lengths | Capabilities                                                                                   | Use Cases                                                                                                         |\n",
    "|---------------|------------|-----------------|------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------|\n",
    "| **Phi-3-Vision** | 4.2 billion | 128K            | A multimodal marvel that seamlessly integrates language and vision.                            | Ideal for interpreting real-world images and digital documents, extracting text from visuals, and analyzing charts and diagrams. |\n",
    "| **Phi-3-Small**  | 7 billion   | 128K, 8K        | A versatile language model excelling in language, reasoning, coding, and math benchmarks.      | Offers unparalleled performance, setting a new standard for efficiency and cost-effectiveness.                    |\n",
    "| **Phi-3-Medium** | 14 billion  | 128K, 4K        | A language model that continues to outshine larger models in understanding and reasoning tasks. | Showcases exceptional capabilities in language understanding, reasoning tasks, and coding benchmarks.             |\n",
    "| **Phi-3-Mini**   | 3.8 billion | 128K, 4K        | Introduced on April 23, 2024, excels in long-context scenarios for its size.                   | Perfect for reasoning tasks and is readily accessible via Models-as-a-Service (MaaS).                             |\n",
    "\n",
    "### üí° Benefits of Phi-3 Models\n",
    "\n",
    "- **Performance and Efficiency:** The Phi-3 family is engineered to surpass models within the same parameter range, delivering top-tier performance in both language and vision tasks across a multitude of applications.\n",
    "- **Cost Efficiency:** Models like Phi-3-mini and Phi-3-small offer budget-friendly solutions without sacrificing performance, ideal for environments where computational resources are limited.\n",
    "- **Versatility:** With capabilities spanning natural language processing, coding, math, and multimodal tasks, the Phi-3 models are adept at handling both general-purpose and specialized applications.\n",
    "\n",
    "Available through Azure AI Studio and MaaS, these models ensure easy integration into projects, with Phi-3-mini and Phi-3-medium being particularly accessible in regions such as East US2 and Sweden Central.\n",
    "\n",
    "### üõ†Ô∏è Support and Availability\n",
    "\n",
    "- **Fine-Tuning Support:** With the exception of Phi-3-vision, all models are fine-tunable using the Azure Machine Learning SDK and UI, allowing for customization to meet specific project needs.\n",
    "- **Global Reach:** Phi-3 models are accessible in all regions supported by Azure AI Studio, ensuring developers worldwide can leverage their advanced capabilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elevating Query Retrieval with Phi-3 Fine-Tuning \n",
    "\n",
    "In this comprehensive guide, we outline our ambitious project to refine the phi-3 model through targeted fine-tuning. Our mission is to supercharge our retrieval system, enabling it to classify user queries with unparalleled precision and select the most effective retrieval strategy. This strategic enhancement is poised to significantly uplift user experience and drive down operational costs.\n",
    "\n",
    "- [Fine-Tuning with Azure Machine Learning Documentation](https://github.com/Azure/azure-llm-fine-tuning/tree/main/fine-tuning)\n",
    "\n",
    "#### üåê Phi-3 Models in the Wild: Versatility at Its Best\n",
    "\n",
    "Phi-3 models stand out for their exceptional adaptability, making them perfect candidates for a myriad of real-world applications:\n",
    "\n",
    "- **Efficiency on the Edge:** Tailored for performance in environments with limited computational resources, these models are a boon for language processing tasks on edge devices.\n",
    "  \n",
    "- **Instant Insights, Anywhere:** Phi-3 models are designed for rapid, local inference, ideal for powering responsive applications in offline settings, such as customer support chatbots.\n",
    "  \n",
    "- **Tailored for Every Domain:** From summarizing extensive documents to generating engaging narratives, phi-3 models are adept at specialized tasks across various industries.\n",
    "\n",
    "#### üéØ Retrieval Revolution: Strategies Unveiled\n",
    "\n",
    "Our retrieval system employs a multifaceted approach to ensure maximum relevance and accuracy across all queries:\n",
    "\n",
    "Please read these for context on the types of issues and studies:\n",
    "- [Azure AI Search: Outperforming Vector Search with Hybrid Approaches](https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/azure-ai-search-outperforming-vector-search-with-hybrid/ba-p/3929167)\n",
    "\n",
    "1. **Keyword Retrieval:** The cornerstone of traditional search, effective for straightforward queries but less so for complex or misspelled inputs.\n",
    "\n",
    "2. **Vector Retrieval:** A modern twist using embeddings to understand semantic similarities, overcoming the limitations of keyword-only searches.\n",
    "\n",
    "3. **Hybrid Retrieval:** A best-of-both-worlds strategy that combines keyword and vector methods to cover all bases, ensuring no query is left behind.\n",
    "\n",
    "4. **Semantic Ranking:** The final touch, reordering results to prioritize relevance, leveraging deep learning models inspired by Microsoft Bing's algorithms.\n",
    "\n",
    "#### üé¢ Strategy and Objectives: Navigating the Path to Excellence\n",
    "\n",
    "Our roadmap is clear: harness the phi-3 model to categorize user queries with razor-sharp accuracy, ensuring each query activates the most suitable retrieval mechanism. This strategy promises to significantly enhance system performance. Key milestones include:\n",
    "\n",
    "- **Crafting the Perfect Dataset:** A curated collection of data designed to boost the phi-3 model's query classification prowess.\n",
    "  \n",
    "- **Building a Superior Retrieval System:** A cutting-edge system that marries keyword and vector search methods with semantic ranking for unmatched relevance.\n",
    "  \n",
    "- **Setting the Performance Bar High:** Rigorous evaluation using leading metrics to ensure our system sets new standards for effectiveness.\n",
    "\n",
    "#### üöß From Blueprint to Reality: The Journey Ahead\n",
    "\n",
    "We're set to embark on a journey of fine-tuning and system integration, with the following phases:\n",
    "\n",
    "1. **Dataset Curation:** Assembling a rich dataset to train the phi-3 model, covering a wide spectrum of query types.\n",
    "   \n",
    "2. **Precision Fine-Tuning:** Employing QLoRA to fine-tune the phi-3 model, enhancing its ability to accurately classify and route queries.\n",
    "   \n",
    "3. **Seamless System Integration:** Incorporating the refined model into our retrieval framework, tailored to adapt to diverse search scenarios.\n",
    "   \n",
    "4. **Benchmarking Success:** Evaluating our system's performance in real-world and controlled environments to confirm our anticipated gains in accuracy, responsiveness, and efficiency.\n",
    "\n",
    "By embarking on this project, we aim to not only elevate the precision and efficiency of our retrieval system but also to redefine the benchmarks for language processing tasks through innovative fine-tuning of the phi-3 model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup Azure Machine Learning (AML) Requirements\n",
    "\n",
    "To establish a connection with an Azure Machine Learning workspace, specific identifying parameters are required: a subscription ID, a resource group name, and a workspace name. These details will be utilized with the `MLClient` from `azure.ai.ml` to access the desired Azure Machine Learning workspace. For authentication, the default Azure credentials will be employed in this hands-on guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m azure_compute_cluster_name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mazure_compute_cluster_name\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     20\u001b[0m azure_compute_cluster_size \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mazure_compute_cluster_size\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 22\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(CLOUD_DIR, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\fine-tuning-transformers\\lib\\os.py:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[1;34m(name, mode, exist_ok)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 225\u001b[0m     \u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exist_ok \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39misdir(name):\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: ''"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Gather values from .env\n",
    "AZURE_SUBSCRIPTION_ID = os.getenv('AZURE_SUBSCRIPTION_ID')\n",
    "AZURE_RESOURCE_GROUP = os.getenv('AZURE_RESOURCE_GROUP')\n",
    "AZURE_WORKSPACE = os.getenv('AZURE_WORKSPACE')\n",
    "AZURE_DATA_NAME = os.getenv('AZURE_DATA_NAME')\n",
    "DATA_DIR = os.getenv('DATA_DIR')\n",
    "CLOUD_DIR = os.getenv('CLOUD_DIR')\n",
    "HF_MODEL_NAME_OR_PATH = os.getenv('HF_MODEL_NAME_OR_PATH')\n",
    "IS_DEBUG = os.getenv('IS_DEBUG') == 'True'  # Assuming IS_DEBUG is a boolean value stored as string\n",
    "USE_LOWPRIORITY_VM = os.getenv('USE_LOWPRIORITY_VM') == 'True'  # Assuming USE_LOWPRIORITY_VM is a boolean value stored as string\n",
    "\n",
    "azure_env_name = os.getenv('azure_env_name')\n",
    "azure_compute_cluster_name = os.getenv('azure_compute_cluster_name')\n",
    "azure_compute_cluster_size = os.getenv('azure_compute_cluster_size')\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(CLOUD_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src. import AMLManager\n",
    "\n",
    "ml_manager = AMLManager("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import time\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "from azure.ai.ml import MLClient, Input\n",
    "from azure.ai.ml.dsl import pipeline\n",
    "from azure.ai.ml import load_component\n",
    "from azure.ai.ml import command\n",
    "from azure.ai.ml.entities import Data, Environment, BuildContext\n",
    "from azure.ai.ml.entities import Model\n",
    "from azure.ai.ml import Input\n",
    "from azure.ai.ml import Output\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from azure.core.exceptions import ResourceNotFoundError, ResourceExistsError\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "ml_client = None\n",
    "try:\n",
    "    ml_client = MLClient.from_config(credential)\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "    ml_client = MLClient(credential, AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP, AZURE_WORKSPACE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {CLOUD_DIR}/conda.yml\n",
    "name: model-env\n",
    "channels:\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.10\n",
    "  - pip=24.0\n",
    "  - pip:\n",
    "    - bitsandbytes==0.43.1\n",
    "    - transformers~=4.41\n",
    "    - peft~=0.11\n",
    "    - accelerate~=0.30\n",
    "    - trl==0.9.4\n",
    "    - einops==0.8.0\n",
    "    - datasets==2.20.0\n",
    "    - wandb==0.17.2\n",
    "    - mlflow==2.14.1\n",
    "    - azureml-mlflow==1.56.0\n",
    "    - azureml-sdk==1.56.0\n",
    "    - torchvision==0.18.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " import os\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "snapshot_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "with open('config.yml') as f:\n",
    "    d = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    \n",
    "AZURE_SUBSCRIPTION_ID = d['config']['AZURE_SUBSCRIPTION_ID']\n",
    "AZURE_RESOURCE_GROUP = d['config']['AZURE_RESOURCE_GROUP']\n",
    "AZURE_WORKSPACE = d['config']['AZURE_WORKSPACE']\n",
    "AZURE_DATA_NAME = d['config']['AZURE_DATA_NAME']    \n",
    "DATA_DIR = d['config']['DATA_DIR']\n",
    "CLOUD_DIR = d['config']['CLOUD_DIR']\n",
    "HF_MODEL_NAME_OR_PATH = d['config']['HF_MODEL_NAME_OR_PATH']\n",
    "IS_DEBUG = d['config']['IS_DEBUG']\n",
    "USE_LOWPRIORITY_VM = d['config']['USE_LOWPRIORITY_VM']\n",
    "\n",
    "azure_env_name = d['train']['azure_env_name']  \n",
    "azure_compute_cluster_name = d['train']['azure_compute_cluster_name']\n",
    "azure_compute_cluster_size = d['train']['azure_compute_cluster_size']\n",
    "\n",
    "!rm -rf $DATA_DIR \n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(CLOUD_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"prompt\":\"Create a step-by-step guide on how to make your own DIY makeup remover using natural ingredients. Include specific measurements and instructions on how to mix the ingredients together. Also, recommend storage options and provide tips on how to use it effectively.\",\"prompt_id\":\"8c24e2e4794d078ef832e0994696de577f8f241ebfb9b6492cddbca43e003385\",\"messages\":[{\"content\":\"Create a step-by-step guide on how to make your own DIY makeup remover using natural ingredients. Include specific measurements and instructions on how to mix the ingredients together. Also, recommend storage options and provide tips on how to use it effectively.\",\"role\":\"user\"}, {\"content\":\"Step-by-Step Guide to Make Your Own DIY Makeup Remover using Natural Ingredients:\\n\\nIngredients:\\n\\n- 1\\/2 cup of extra-virgin olive oil\\n- 1\\/4 cup of witch hazel\\n- 1\\/4 cup of filtered water\\n- 1 tablespoon of castile soap\\n- 5 drops of tea tree oil\\n\\nTools Required:\\n\\n- A mixing bowl\\n- A stirring spoon\\n- A glass container for storage\\n\\nInstructions:\\n\\n1. In a mixing bowl, add 1\\/2 cup of extra-virgin olive oil.\\n\\n2. Add 1\\/4 cup of witch hazel to the mixing bowl and stir well.\\n\\n3. Add 1\\/4 cup of filtered water to the mixing bowl and stir until the ingredients are well combined.\\n\\n4. Add 1 tablespoon of castile soap to the mixing bowl and stir gently to combine.\\n\\n5. Add 5 drops of tea tree oil to the mixture and stir.\\n\\n6. Transfer the mixture to a glass container for storage.\\n\\n7. Label the container with the date and ingredients used.\\n\\nTips on Using Your DIY Makeup Remover:\\n\\n- Shake well before using to ensure the ingredients are mixed.\\n- Use a cotton pad to apply the mixture to your face.\\n- Rub the cotton pad gently in circular motions over your face to remove makeup.\\n- Wash your face with warm water after using the makeup remover.\\n- Store your DIY makeup remover in a cool, dry, and dark place to avoid spoiling.\\n- Try using different essential oils, such as lavender or chamomile, to customize your makeup remover according to your skin type.\\n- You can also use this makeup remover as a facial cleanser to remove dirt and impurities from your face.\",\"role\":\"assistant\"},{\"content\":\"This guide is great! Can you recommend some specific brands for the ingredients, such as the castile soap and witch hazel? And also, can you add some tips on how to remove stubborn eye makeup with this DIY makeup remover?\",\"role\":\"user\"},{\"content\":\"Yes, definitely! Here are some recommendations for brands of the ingredients:\\n\\n- Castile soap: Dr. Bronner's Castile Soap\\n- Witch hazel: Thayers Witch Hazel Toner\\n- Tea tree oil: Healing Solutions Tea Tree Essential Oil\\n\\nTips on Removing Stubborn Eye Makeup:\\n\\n- Soak a cotton pad with the DIY makeup remover and hold it over your closed eye for a few seconds to allow the makeup to dissolve.\\n- Gently swipe the cotton pad over your eyelids and eyelashes, using small circular motions.\\n- For particularly stubborn eye makeup, you can use a Q-tip soaked in the DIY makeup remover to remove any remnants.\\n- Be careful not to rub the cotton pad too hard on your eyes, as this can cause irritation or damage to your delicate eye area.\",\"role\":\"assistant\"}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\fine-tuning-transformers\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading readme: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.44k/4.44k [00:00<?, ?B/s]\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 244M/244M [00:04<00:00, 51.4MB/s] \n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 244M/244M [00:04<00:00, 59.3MB/s] \n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 244M/244M [00:03<00:00, 61.1MB/s] \n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81.2M/81.2M [00:01<00:00, 58.9MB/s]\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 244M/244M [00:04<00:00, 58.5MB/s] \n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 243M/243M [00:04<00:00, 58.9MB/s] \n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 243M/243M [00:04<00:00, 57.3MB/s] \n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 80.4M/80.4M [00:01<00:00, 55.8MB/s]\n",
      "Generating train_sft split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 207865/207865 [00:05<00:00, 36322.25 examples/s]\n",
      "Generating test_sft split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23110/23110 [00:00<00:00, 37195.72 examples/s]\n",
      "Generating train_gen split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 256032/256032 [00:05<00:00, 45348.03 examples/s]\n",
      "Generating test_gen split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28304/28304 [00:00<00:00, 44870.21 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: 4157\n",
      "{'prompt': 'Write a step-by-step guide on how to set up and effectively use Trello for managing a project, including how to create boards, lists, and cards, assign tasks to team members, track progress, and collaborate efficiently. Provide tips on best practices for labeling, tagging, and organizing information within Trello, as well as suggestions for integrating other project management tools or software.', 'prompt_id': '08e593adfb879510d70e6065a38e605a9199a6686d7ab8a44d541aa30e5268ab', 'messages': [{'content': 'Write a step-by-step guide on how to set up and effectively use Trello for managing a project, including how to create boards, lists, and cards, assign tasks to team members, track progress, and collaborate efficiently. Provide tips on best practices for labeling, tagging, and organizing information within Trello, as well as suggestions for integrating other project management tools or software.', 'role': 'user'}, {'content': 'Step-by-Step Guide on how to set up and effectively use Trello for managing a project:\\n\\n1. Sign up for a Trello account: Go to the Trello website and sign up for an account with your email address.\\n\\n2. Create a board: After signing up, create a board by clicking the (+) icon in the upper right corner of the Trello dashboard.  Name your board with a descriptive title relevant to your project. \\n\\n3. Create lists: A Trello board is made of lists. Create a list to represent each stage of your project such as \"To Do\", \"In Progress\", \"Complete\", etc.\\n\\n4. Add cards: Each card represents a task in your project. Click \"Add a card\" to create a card for each task in your \"To Do\" list. Add as much detail as you can, and use the comment section to add further notes.\\n\\n5. Assign tasks to team members: Click on the card to open it, and you\\'ll be able to assign the task to yourself or any member of your team.\\n\\n6. Track progress: As tasks are completed move them to the \"Completed\" list. Use labels [board filters] to identify priority, type of task or to add more information about the task.\\n\\n7. Collaborate efficiently: Enable commenting on the board and on each card.  Members can make comments on the progress of a task, and manage text communication within the tool.\\n\\n8. Best Practices:\\n\\nLabeling- Labels aren‚Äôt just useful for marking progress. They can also be used to denote different types of tasks, assign priority, or even identify which team member is responsible for the task.\\n\\nTagging: Using tags like #design, #development or #marketing for different types of tasks, will aid in quickly defining what\\'s been done and what needs to be done.\\n\\nOrganization: Use the \"Custom Fields\" and \"Power-Ups\" options in the Trello menu to add more details about tasks and team members.\\n\\nIntegration with other software: Trello offers a lot of integrations that can work seamlessly together. Try to use it with software like G-Suite, Slack, IFTTT for a more efficient workflow.\\n\\nImplementing these best practices can help to simplify complex projects, streamline team communication, and ensure successful project completion.', 'role': 'assistant'}, {'content': 'Thanks for the step-by-step guide on how to use Trello for managing a project. Can you please add more information on how to use the calendar view in Trello and how to integrate it with other productivity tools like Google Calendar?', 'role': 'user'}, {'content': \"Yes, absolutely! Here's more information on using the calendar view in Trello and integrating it with other productivity tools:\\n\\nUsing the calendar view in Trello:\\nTrello has a calendar view that can be very helpful in managing your projects. Here is how to access and use the Trello calendar view:\\n\\n1. Open your Trello project board and click the ‚ÄúShow Menu‚Äù button.\\n\\n2. From the menu, select ‚ÄúPower-Ups‚Äù, then search for ‚ÄúCalendar‚Äù and enable it.\\n\\n3. Once the Calendar Power-Up is enabled, you will see a new ‚ÄúCalendar‚Äù option in the menu. Click on ‚ÄúCalendar‚Äù to see a calendar view of all the cards on your board.\\n\\n4. You can also filter cards by labels and assign due dates to cards so that they show up on specific dates.\\n\\n5. Drag and drop cards on the calendar to schedule and reschedule tasks as needed.\\n\\nIntegrating Trello with Google Calendar:\\nYou can also integrate Trello with Google Calendar to help you stay on top of your deadlines and easily manage your schedule.\\n\\n1. To integrate Trello and Google Calendar, open your Trello account and click on the three dots in the top right corner of the calendar.\\n\\n2. From the drop-down menu, select ‚ÄúCalendar Feed‚Äù and copy the link provided.\\n\\n3. Open your Google Calendar and click the ‚Äú+‚Äù icon next to ‚ÄúOther calendars‚Äù on the left-hand panel.\\n\\n4. From the drop-down menu, select ‚ÄúFrom URL‚Äù.\\n\\n5. Paste the Trello Calendar Feed link into the URL field and click ‚ÄúAdd Calendar.‚Äù\\n\\n6. Now all of the due dates from your Trello board will be displayed on your Google Calendar.\\n\\nBy following these steps, you can integrate Trello and Google Calendar to have a comprehensive view of all your deadlines in one place.\", 'role': 'assistant'}, {'content': 'Thanks for the additional information on using the calendar view in Trello and integrating it with Google Calendar. Can you also provide some tips on how to effectively use the checklist feature in Trello to manage tasks within a card? And maybe some examples of how to use the checklist for different types of tasks?', 'role': 'user'}, {'content': 'Yes, definitely! Here are some tips on how to effectively use the checklist feature in Trello to manage tasks within a card, along with some examples of how it can be used for different types of tasks:\\n\\n1. Break down large tasks into smaller sub-tasks: A checklist is a great way to break down a large task into smaller, more manageable sub-tasks. For example, if you have a task to launch a new website, you can create a checklist with sub-tasks like \"Choose a domain name,\" \"Design the website layout,\" \"Create content,\" \"Optimize for search engines,\" and \"Launch the site.\"\\n\\n2. Prioritize tasks: You can use checklists to prioritize tasks within a card. For example, if you have a card for \"Blog Posts,\" you can create a checklist for each post and prioritize them based on their importance or deadline.\\n\\n3. Keep track of progress: Adding a checklist to a task can help you keep track of its progress. You can mark items as complete as you work through them, making it easier to see what\\'s left to do.\\n\\n4. Delegate tasks: You can use the checklist feature to delegate tasks to your team members. For example, if you have a task to create a presentation, you can create a checklist with sub-tasks for each team member responsible for different parts of the presentation.\\n\\n5. Add notes and comments: You can use the checklist to add notes and comments about each task. For example, if you have a task to write a blog post, you can add a checklist item for research and add notes about what resources you\\'d like to use.\\n\\n6. Use multiple checklists: If your card has a lot of tasks, you can create multiple checklists to organize them further. For example, if you have a card for \"Marketing Campaign,\" you could create separate checklists for \"Social Media,\" \"Email Marketing,\" and \"Advertising.\"\\n\\nOverall, the checklist feature in Trello is a great tool for breaking down tasks, prioritizing work, keeping track of progress, and collaborating with your team. By following these tips, you can effectively use the checklist feature to manage tasks within your cards, and improve your productivity and efficiency.', 'role': 'assistant'}, {'content': \"Thank you so much for the detailed information on how to use Trello effectively. Can you please provide more information on how to use Trello's calendar view with other productivity tools or software, such as Asana or Microsoft Teams? Additionally, can you recommend any useful Trello integrations that can help streamline project management further? I appreciate your help!\", 'role': 'user'}, {'content': \"Yes, absolutely! Here is more information on how to use Trello's calendar view with other productivity tools, as well as some useful integrations that can help streamline project management even further:\\n\\nUsing Trello's calendar view with other productivity tools:\\nTrello's calendar view can be used in conjunction with other productivity tools to create an even more streamlined project management system. Here are some examples of how you could use Trello's calendar view with other popular productivity tools:\\n\\n1. Asana - Asana is another project management tool that offers a calendar view. You can link your Trello boards to your Asana projects and use the calendar view in both tools to get an overview of all your deadlines and tasks.\\n\\n2. Microsoft Teams - Microsoft Teams is a collaborative workspace tool. By linking Trello to Microsoft Teams, you can display Trello cards and boards directly within Teams channels. Additionally, by adding Trello's calendar view to your Teams calendars, you can see your upcoming deadlines and tasks from within Teams.\\n\\nUseful Trello integrations:\\nThere are many useful Trello integrations that can help streamline your project management system even further. Here are some of the most popular integrations:\\n\\n1. Slack - Slack is a team communication tool that can integrate with Trello. You can use the Trello power-up in Slack to create Trello cards and receive Trello notifications directly within Slack.\\n\\n2. Zapier - Zapier is an automation tool that can help you automate repetitive tasks between Trello and other apps. You can use Zapier to automatically create Trello cards when new tasks are added to your project management system, or to create Trello cards from new emails in your inbox.\\n\\n3. Google Drive - By linking Trello to Google Drive, you can attach Google Drive files directly to Trello cards. This is a great way to store and share important project-related documents.\\n\\n4. Evernote - Evernote is a note-taking app that can integrate with Trello. You can use the Trello power-up in Evernote to create Trello cards and attach Evernote notes to Trello cards.\\n\\nBy using these Trello integrations, you can create a more streamlined project management system, automate repetitive tasks, and make it easier to share and store important project-related documents.\", 'role': 'assistant'}]}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"HuggingFaceH4/ultrachat_200k\", split='train_sft[:2%]')\n",
    "\n",
    "print(f\"dataset size: {len(dataset)}\")\n",
    "print(dataset[randrange(len(dataset))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  2.80ba/s]\n",
      "Creating json from Arrow format: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.46ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5803139"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = dataset['train']\n",
    "train_dataset.to_json(f\"data/train.jsonl\")\n",
    "test_dataset = dataset['test']\n",
    "test_dataset.to_json(f\"data/eval.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating training script "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/finetuning/phi3/train_mlflow.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/finetuning/phi3/train_mlflow.py\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.types import DataType, Schema, ColSpec\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "\n",
    "import os\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "import argparse\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "import torch\n",
    "import transformers\n",
    "from trl import SFTTrainer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "from datetime import datetime\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_model(model_name_or_path: str = \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "               use_cache: bool = False,\n",
    "               trust_remote_code: bool = True,\n",
    "               torch_dtype: torch.dtype = torch.bfloat16,\n",
    "               device_map: dict = None,\n",
    "               max_seq_length: int = 4096) -> tuple:\n",
    "    \"\"\"\n",
    "    Loads a pre-trained model and its tokenizer with specified configurations.\n",
    "\n",
    "    Parameters:\n",
    "    - model_name_or_path (str): Identifier for the model to load. Can be a model ID or path.\n",
    "    - use_cache (bool): Whether to use caching for model outputs.\n",
    "    - trust_remote_code (bool): Whether to trust remote code when loading the model.\n",
    "    - torch_dtype (torch.dtype): Data type for model tensors. Recommended to use torch.bfloat16 for efficiency.\n",
    "    - device_map (dict): Custom device map for distributing the model's layers across devices.\n",
    "    - max_seq_length (int): Maximum sequence length for the tokenizer.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple containing the loaded model and tokenizer.\n",
    "    \"\"\"\n",
    "    model_kwargs = {\n",
    "        \"use_cache\": use_cache,\n",
    "        \"trust_remote_code\": trust_remote_code,\n",
    "        \"torch_dtype\": torch_dtype,\n",
    "        \"device_map\": device_map\n",
    "    }\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name_or_path, **model_kwargs)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "    tokenizer.model_max_length = max_seq_length\n",
    "    tokenizer.pad_token = tokenizer.unk_token  # use unk rather than eos token to prevent endless generation\n",
    "    tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    return model, tokenizer\n",
    "\n",
    "def convert_to_chat_format(df: pd.DataFrame) -> List[List[Dict[str, str]]]:\n",
    "    \"\"\"\n",
    "    Converts a DataFrame containing questions and their types into a chat format.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): A DataFrame with at least two columns: 'Question' and 'Kind of Query'.\n",
    "      Each row represents a user query and its categorization.\n",
    "\n",
    "    Returns:\n",
    "    - List[List[Dict[str, str]]]: A list of chats, where each chat is a list of messages.\n",
    "      Each message is a dictionary with 'role' and 'content' keys. The 'role' can be 'system',\n",
    "      'user', or 'assistant', indicating the sender of the message. The 'content' is the text of the message.\n",
    "    \"\"\"\n",
    "    chats = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        chat = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an AI assistant supporting users by categorizing their queries.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": row[\"Question\"]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": f\"This query is a '{row['Kind of Query']}' type.\"\n",
    "            }\n",
    "        ]\n",
    "        chats.append(chat)\n",
    "    \n",
    "    return chats\n",
    "\n",
    "def convert_chats_to_dataframe(chats: List[List[Dict[str, str]]]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Converts a list of chats into a DataFrame where each chat is represented as a dictionary in the 'message' column.\n",
    "\n",
    "    Parameters:\n",
    "    - chats (List[List[Dict[str, str]]]): A list of chats, where each chat is a list of messages.\n",
    "      Each message is a dictionary with 'role' and 'content' keys.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame with a single column 'message', where each row contains a dictionary\n",
    "      representing a chat.\n",
    "    \"\"\"\n",
    "    # Convert each chat into a dictionary and store it in a list\n",
    "    chat_dicts = [{'message': chat} for chat in chats]\n",
    "    \n",
    "    # Create a DataFrame from the list of dictionaries\n",
    "    df = pd.DataFrame(chat_dicts)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def apply_chat_template(\n",
    "    example: dict,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Applies a chat template to the messages in an example from a dataset.\n",
    "\n",
    "    This function modifies the input example by adding a system message at the beginning\n",
    "    if it does not already start with one. It then applies a chat template formatting\n",
    "    using the specified tokenizer.\n",
    "\n",
    "    Parameters:\n",
    "    - example (dict): A dictionary representing a single example from a dataset. It must\n",
    "      contain a key 'messages', which is a list of message dictionaries. Each message\n",
    "      dictionary should have 'role' and 'content' keys.\n",
    "    - tokenizer (PreTrainedTokenizer): An instance of a tokenizer that supports the\n",
    "      `apply_chat_template` method for formatting chat messages.\n",
    "\n",
    "    Returns:\n",
    "    - dict: The modified example dictionary with an added 'text' key that contains the\n",
    "      formatted chat as a string.\n",
    "    \"\"\"\n",
    "    messages = example[\"message\"]\n",
    "    # Add an empty system message if there is none\n",
    "    if messages[0][\"role\"] != \"system\":\n",
    "        messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n",
    "    example[\"text\"] = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=False)\n",
    "    return example\n",
    "\n",
    "def main(args):\n",
    "    \n",
    "    ###################\n",
    "    # Hyper-parameters\n",
    "    ###################\n",
    "    # Only overwrite environ if wandb param passed\n",
    "    if len(args.wandb_project) > 0:\n",
    "        os.environ['WANDB_API_KEY'] = args.wandb_api_key    \n",
    "        os.environ[\"WANDB_PROJECT\"] = args.wandb_project\n",
    "    if len(args.wandb_watch) > 0:\n",
    "        os.environ[\"WANDB_WATCH\"] = args.wandb_watch\n",
    "    if len(args.wandb_log_model) > 0:\n",
    "        os.environ[\"WANDB_LOG_MODEL\"] = args.wandb_log_model\n",
    "\n",
    "    use_wandb = len(args.wandb_project) > 0 or (\"WANDB_PROJECT\" in os.environ and len(os.environ[\"WANDB_PROJECT\"]) > 0) \n",
    "\n",
    "    training_config = {\n",
    "        \"bf16\": True,\n",
    "        \"do_eval\": False,\n",
    "        \"learning_rate\": args.learning_rate,\n",
    "        \"log_level\": \"info\",\n",
    "        \"logging_steps\": args.logging_steps,\n",
    "        \"logging_strategy\": \"steps\",\n",
    "        \"lr_scheduler_type\": args.lr_scheduler_type,\n",
    "        \"num_train_epochs\": args.epochs,\n",
    "        \"max_steps\": -1,\n",
    "        \"output_dir\": args.output_dir,\n",
    "        \"overwrite_output_dir\": True,\n",
    "        \"per_device_train_batch_size\": args.train_batch_size,\n",
    "        \"per_device_eval_batch_size\": args.eval_batch_size,\n",
    "        \"remove_unused_columns\": True,\n",
    "        \"save_steps\": args.save_steps,\n",
    "        \"save_total_limit\": 1,\n",
    "        \"seed\": args.seed,\n",
    "        \"gradient_checkpointing\": True,\n",
    "        \"gradient_checkpointing_kwargs\": {\"use_reentrant\": False},\n",
    "        \"gradient_accumulation_steps\": args.grad_accum_steps,\n",
    "        \"warmup_ratio\": args.warmup_ratio,\n",
    "    }\n",
    "\n",
    "    peft_config = {\n",
    "        \"r\": args.lora_r,\n",
    "        \"lora_alpha\": args.lora_alpha,\n",
    "        \"lora_dropout\": args.lora_dropout,\n",
    "        \"bias\": \"none\",\n",
    "        \"task_type\": \"CAUSAL_LM\",\n",
    "        #\"target_modules\": \"all-linear\",\n",
    "        \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        \"modules_to_save\": None,\n",
    "    }\n",
    "\n",
    "    checkpoint_dir = os.path.join(args.output_dir, \"checkpoints\")\n",
    "\n",
    "    train_conf = TrainingArguments(\n",
    "        **training_config,\n",
    "        report_to=\"wandb\" if use_wandb else \"azure_ml\",\n",
    "        run_name=args.wandb_run_name if use_wandb else None,    \n",
    "    )\n",
    "    peft_conf = LoraConfig(**peft_config)\n",
    "    model, tokenizer = load_model(args)\n",
    "\n",
    "    ###############\n",
    "    # Setup logging\n",
    "    ###############\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "        handlers=[logging.StreamHandler(sys.stdout)],\n",
    "    )\n",
    "    log_level = train_conf.get_process_log_level()\n",
    "    logger.setLevel(log_level)\n",
    "    datasets.utils.logging.set_verbosity(log_level)\n",
    "    transformers.utils.logging.set_verbosity(log_level)\n",
    "    transformers.utils.logging.enable_default_handler()\n",
    "    transformers.utils.logging.enable_explicit_format()\n",
    "\n",
    "    # Log on each process a small summary\n",
    "    logger.warning(\n",
    "        f\"Process rank: {train_conf.local_rank}, device: {train_conf.device}, n_gpu: {train_conf.n_gpu}\"\n",
    "        + f\" distributed training: {bool(train_conf.local_rank != -1)}, 16-bits training: {train_conf.fp16}\"\n",
    "    )\n",
    "    logger.info(f\"Training/evaluation parameters {train_conf}\")\n",
    "    logger.info(f\"PEFT parameters {peft_conf}\")    \n",
    "\n",
    "    ##################\n",
    "    # Data Processing\n",
    "    ##################\n",
    "\n",
    "    train_dataset = load_dataset('json', data_files=os.path.join(args.train_dir, 'train.jsonl'), split='train')\n",
    "    eval_dataset = load_dataset('json', data_files=os.path.join(args.train_dir, 'eval.jsonl'), split='train')\n",
    "    column_names = list(train_dataset.features)\n",
    "\n",
    "    train_data = pd.read_csv(args.train_dir)\n",
    "    train_data_chat_format = convert_to_chat_format(train_data)\n",
    "    df_train_data_chat_format = convert_chats_to_dataframe(train_data_chat_format)\n",
    "    train_dataset = datasets.Dataset.from_pandas(pd.DataFrame(df_train_data_chat_format, columns=[\"message\"]),split= \"train\")\n",
    "\n",
    "    eval_data = pd.read_csv(args.eval_data)\n",
    "    eval_data_chat_format = convert_to_chat_format(eval_data)\n",
    "    df_eval_data_chat_format = convert_chats_to_dataframe(eval_data_chat_format)\n",
    "    eval_data_dataset = datasets.Dataset.from_pandas(pd.DataFrame(df_eval_data_chat_format, columns=[\"message\"]),split=\"train\")\n",
    "\n",
    "    column_names = list(train_dataset.features)\n",
    "\n",
    "    processed_train_dataset = train_dataset.map(\n",
    "        apply_chat_template,\n",
    "        fn_kwargs={\"tokenizer\": tokenizer},\n",
    "        num_proc=10,\n",
    "        remove_columns=column_names,\n",
    "        desc=\"Applying chat template to train_sft\",\n",
    "    )\n",
    "\n",
    "    processed_eval_dataset = eval_dataset.map(\n",
    "        apply_chat_template,\n",
    "        fn_kwargs={\"tokenizer\": tokenizer},\n",
    "        num_proc=10,\n",
    "        remove_columns=column_names,\n",
    "        desc=\"Applying chat template to test_sft\",\n",
    "    )\n",
    "\n",
    "    with mlflow.start_run() as run:        \n",
    "        ###########\n",
    "        # Training\n",
    "        ###########\n",
    "        trainer = SFTTrainer(\n",
    "            model=model,\n",
    "            args=train_conf,\n",
    "            peft_config=peft_conf,\n",
    "            train_dataset=processed_train_dataset,\n",
    "            eval_dataset=processed_eval_dataset,\n",
    "            max_seq_length=args.max_seq_length,\n",
    "            dataset_text_field=\"text\",\n",
    "            tokenizer=tokenizer,\n",
    "            packing=True,\n",
    "        )\n",
    "\n",
    "        # Show current memory stats\n",
    "        gpu_stats = torch.cuda.get_device_properties(0)\n",
    "        start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "        max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "        logger.info(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "        logger.info(f\"{start_gpu_memory} GB of memory reserved.\")\n",
    "        \n",
    "        last_checkpoint = None\n",
    "        if os.path.isdir(checkpoint_dir):\n",
    "            checkpoints = [os.path.join(checkpoint_dir, d) for d in os.listdir(checkpoint_dir)]\n",
    "            if len(checkpoints) > 0:\n",
    "                checkpoints.sort(key=os.path.getmtime, reverse=True)\n",
    "                last_checkpoint = checkpoints[0]        \n",
    "\n",
    "        trainer_stats = trainer.train(resume_from_checkpoint=last_checkpoint)\n",
    "\n",
    "        #############\n",
    "        # Logging\n",
    "        #############\n",
    "        metrics = trainer_stats.metrics\n",
    "\n",
    "        # Show final memory and time stats \n",
    "        used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "        used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "        used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "        lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "\n",
    "        logger.info(f\"{metrics['train_runtime']} seconds used for training.\")\n",
    "        logger.info(f\"{round(metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "        logger.info(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "        logger.info(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "        logger.info(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "        logger.info(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")\n",
    "                \n",
    "        trainer.log_metrics(\"train\", metrics)\n",
    "\n",
    "        model_info = mlflow.transformers.log_model(\n",
    "            transformers_model={\"model\": trainer.model, \"tokenizer\": tokenizer},\n",
    "            #prompt_template=prompt_template,\n",
    "            #signature=signature,\n",
    "            artifact_path=args.model_dir,  # This is a relative path to save model files within MLflow run\n",
    "        )\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    # setup argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # curr_time = datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "\n",
    "    # hyperparameters\n",
    "    parser.add_argument(\"--train_dir\", default=\"data\", type=str, help=\"Input directory for training\")\n",
    "    parser.add_argument(\"--model_dir\", default=\"./model\", type=str, help=\"output directory for model\")\n",
    "    parser.add_argument(\"--epochs\", default=1, type=int, help=\"number of epochs\")\n",
    "    parser.add_argument(\"--output_dir\", default=\"./output_dir\", type=str, help=\"directory to temporarily store when training a model\")\n",
    "    parser.add_argument(\"--train_batch_size\", default=8, type=int, help=\"training - mini batch size for each gpu/process\")\n",
    "    parser.add_argument(\"--eval_batch_size\", default=8, type=int, help=\"evaluation - mini batch size for each gpu/process\")\n",
    "    parser.add_argument(\"--learning_rate\", default=5e-06, type=float, help=\"learning rate\")\n",
    "    parser.add_argument(\"--logging_steps\", default=2, type=int, help=\"logging steps\")\n",
    "    parser.add_argument(\"--save_steps\", default=100, type=int, help=\"save steps\")    \n",
    "    parser.add_argument(\"--grad_accum_steps\", default=4, type=int, help=\"gradient accumulation steps\")\n",
    "    parser.add_argument(\"--lr_scheduler_type\", default=\"linear\", type=str)\n",
    "    parser.add_argument(\"--seed\", default=0, type=int, help=\"seed\")\n",
    "    parser.add_argument(\"--warmup_ratio\", default=0.2, type=float, help=\"warmup ratio\")\n",
    "    parser.add_argument(\"--max_seq_length\", default=2048, type=int, help=\"max seq length\")\n",
    "    parser.add_argument(\"--save_merged_model\", type=bool, default=False)\n",
    "\n",
    "    # lora hyperparameters\n",
    "    parser.add_argument(\"--lora_r\", default=16, type=int, help=\"lora r\")\n",
    "    parser.add_argument(\"--lora_alpha\", default=16, type=int, help=\"lora alpha\")\n",
    "    parser.add_argument(\"--lora_dropout\", default=0.05, type=float, help=\"lora dropout\")\n",
    "    \n",
    "    # wandb params\n",
    "    parser.add_argument(\"--wandb_api_key\", type=str, default=\"\")\n",
    "    parser.add_argument(\"--wandb_project\", type=str, default=\"\")\n",
    "    parser.add_argument(\"--wandb_run_name\", type=str, default=\"\")\n",
    "    parser.add_argument(\"--wandb_watch\", type=str, default=\"gradients\") # options: false | gradients | all\n",
    "    parser.add_argument(\"--wandb_log_model\", type=str, default=\"false\") # options: false | true\n",
    "\n",
    "    # parse args\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # return args\n",
    "    return args\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #sys.argv = ['']\n",
    "    args = parse_args()\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the compute Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "snapshot_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "with open('config.yml') as f:\n",
    "    d = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    \n",
    "AZURE_SUBSCRIPTION_ID = d['config']['AZURE_SUBSCRIPTION_ID']\n",
    "AZURE_RESOURCE_GROUP = d['config']['AZURE_RESOURCE_GROUP']\n",
    "AZURE_WORKSPACE = d['config']['AZURE_WORKSPACE']\n",
    "AZURE_DATA_NAME = d['config']['AZURE_DATA_NAME']    \n",
    "DATA_DIR = d['config']['DATA_DIR']\n",
    "CLOUD_DIR = d['config']['CLOUD_DIR']\n",
    "HF_MODEL_NAME_OR_PATH = d['config']['HF_MODEL_NAME_OR_PATH']\n",
    "IS_DEBUG = d['config']['IS_DEBUG']\n",
    "USE_LOWPRIORITY_VM = d['config']['USE_LOWPRIORITY_VM']\n",
    "\n",
    "azure_env_name = d['train']['azure_env_name']  \n",
    "azure_compute_cluster_name = d['train']['azure_compute_cluster_name']\n",
    "azure_compute_cluster_size = d['train']['azure_compute_cluster_size']\n",
    "\n",
    "!rm -rf $DATA_DIR \n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(CLOUD_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import AmlCompute\n",
    "\n",
    "### Create the compute cluster\n",
    "try:\n",
    "    compute = ml_client.compute.get(azure_compute_cluster_name)\n",
    "    print(\"The compute cluster already exists! Reusing it for the current run\")\n",
    "except Exception as ex:\n",
    "    print(\n",
    "        f\"Looks like the compute cluster doesn't exist. Creating a new one with compute size {azure_compute_cluster_size}!\"\n",
    "    )\n",
    "    try:\n",
    "        print(\"Attempt #1 - Trying to create a dedicated compute\")\n",
    "        tier = 'LowPriority' if USE_LOWPRIORITY_VM else 'Dedicated'\n",
    "        compute = AmlCompute(\n",
    "            name=azure_compute_cluster_name,\n",
    "            size=azure_compute_cluster_size,\n",
    "            tier=tier,\n",
    "            max_instances=1,  # For multi node training set this to an integer value more than 1\n",
    "        )\n",
    "        ml_client.compute.begin_create_or_update(compute).wait()\n",
    "    except Exception as e:\n",
    "        print(\"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = get_or_create_environment_asset(ml_client, azure_env_name, conda_yml=f\"{CLOUD_DIR}/conda.yml\", update=False)\n",
    "data = get_or_create_data_asset(ml_client, AZURE_DATA_NAME, data_local_dir=DATA_DIR, update=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vector-indexing-azureaisearch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
