{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Prerequisites\n",
    "\n",
    "Before starting, ensure your Azure Services are operational, your Conda environment is configured, and your environment variables are set as described in the [README.md](README.md) document.\n",
    "\n",
    "## üìã Table of Contents\n",
    "\n",
    "This notebook provides a practical guide to enhancing the relevance of Phi-3 models through fine-tuning with Azure Machine Learning:\n",
    "\n",
    "1. [**Introduction to Fine-Tuning**](#define-field-types): Delve into the essentials of fine-tuning and Retrieval Augmented Generation (RAG) for Phi-3 models. This section covers their importance, benefits, and the strategic approach to customizing language models, offering insights into their technical aspects and real-world applications.\n",
    "2. [**Exploring the Phi-3 Model Universe**](#configuring-vector-search): Exploring the expanded Phi-3 model family\n",
    "3. [**Use Case: Enhancing Query Retrieval with Phi-3 Fine-Tuning**](#configuring-semantic-search): Learn how fine-tuned SLM models can drastically enhance search capabilities, resulting in a more efficient and accurate retrieval system.\n",
    "\n",
    "For additional information, refer to the following resources:\n",
    "- [Phi-3 Release Documentation](https://azure.microsoft.com/en-us/blog/introducing-phi-3-redefining-whats-possible-with-slms/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory changed to C:\\Users\\pablosal\\Desktop\\gbba-ai-small-language-models\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the target directory\n",
    "target_directory = r\"C:\\Users\\pablosal\\Desktop\\gbba-ai-small-language-models\"  # change your directory here\n",
    "\n",
    "# Check if the directory exists\n",
    "if os.path.exists(target_directory):\n",
    "    # Change the current working directory\n",
    "    os.chdir(target_directory)\n",
    "    print(f\"Directory changed to {os.getcwd()}\")\n",
    "else:\n",
    "    print(f\"Directory {target_directory} does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Phi-3 Model Universe\n",
    "\n",
    "Before we dive in, I highly recommend getting familiar with the nuances of fine-tuning Small Language Models (SLMs), especially focusing on the Phi-3 model. This deep dive will not only introduce you to the flexibility of SLMs in managing real-time applications and niche projects but also showcase how they can be tailored for custom solutions. For a comprehensive guide, check out: [Task Adaptation in Small Language Models: Fine-Tuning Phi-3](https://pabloaicorner.hashnode.dev/task-adaptation-in-small-language-models-fine-tuning-phi-3).\n",
    "\n",
    "Looking for more resources? Don't miss the [Fine-Tuning with Azure Machine Learning Documentation](https://github.com/Azure/azure-llm-fine-tuning/tree/main/fine-tuning) for additional insights and guidance.\n",
    "\n",
    "### Phi-3 Version \n",
    "\n",
    "| Model         | Parameters | Context Lengths | Capabilities                                                                                   | Use Cases                                                                                                         |\n",
    "|---------------|------------|-----------------|------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------|\n",
    "| **Phi-3-Vision** | 4.2 billion | 128K            | A multimodal marvel that seamlessly integrates language and vision.                            | Ideal for interpreting real-world images and digital documents, extracting text from visuals, and analyzing charts and diagrams. |\n",
    "| **Phi-3-Small**  | 7 billion   | 128K, 8K        | A versatile language model excelling in language, reasoning, coding, and math benchmarks.      | Offers unparalleled performance, setting a new standard for efficiency and cost-effectiveness.                    |\n",
    "| **Phi-3-Medium** | 14 billion  | 128K, 4K        | A language model that continues to outshine larger models in understanding and reasoning tasks. | Showcases exceptional capabilities in language understanding, reasoning tasks, and coding benchmarks.             |\n",
    "| **Phi-3-Mini**   | 3.8 billion | 128K, 4K        | Introduced on April 23, 2024, excels in long-context scenarios for its size.                   | Perfect for reasoning tasks and is readily accessible via Models-as-a-Service (MaaS).                             |\n",
    "\n",
    "### üí° Benefits of Phi-3 Models\n",
    "\n",
    "- **Performance and Efficiency:** The Phi-3 family is engineered to surpass models within the same parameter range, delivering top-tier performance in both language and vision tasks across a multitude of applications.\n",
    "- **Cost Efficiency:** Models like Phi-3-mini and Phi-3-small offer budget-friendly solutions without sacrificing performance, ideal for environments where computational resources are limited.\n",
    "- **Versatility:** With capabilities spanning natural language processing, coding, math, and multimodal tasks, the Phi-3 models are adept at handling both general-purpose and specialized applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case: Enhancing Query Retrieval through Phi-3 Fine-Tuning\n",
    "\n",
    "Our goal is to elevate our query retrieval system to new heights, empowering it to categorize user queries with exceptional accuracy and adopt the optimal retrieval strategy. By implementing this advanced fine-tuning, we aim to dramatically improve the user experience while simultaneously reducing operational expenses.\n",
    "\n",
    "\n",
    "#### üåê Phi-3 Models in the Wild: Versatility at Its Best\n",
    "\n",
    "Phi-3 models stand out for their exceptional adaptability, making them perfect candidates for a myriad of real-world applications:\n",
    "\n",
    "- **Efficiency on the Edge:** Tailored for performance in environments with limited computational resources, these models are a boon for language processing tasks on edge devices.\n",
    "  \n",
    "- **Instant Insights, Anywhere:** Phi-3 models are designed for rapid, local inference, ideal for powering responsive applications in offline settings, such as customer support chatbots.\n",
    "  \n",
    "- **Tailored for Every Domain:** From summarizing extensive documents to generating engaging narratives, phi-3 models are adept at specialized tasks across various industries.\n",
    "\n",
    "#### üéØ Retrieval Revolution: Strategies Unveiled\n",
    "\n",
    "Our retrieval system employs a multifaceted approach to ensure maximum relevance and accuracy across all queries:\n",
    "\n",
    "Please read these for context on the types of issues and studies:\n",
    "- [Azure AI Search: Outperforming Vector Search with Hybrid Approaches](https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/azure-ai-search-outperforming-vector-search-with-hybrid/ba-p/3929167)\n",
    "\n",
    "1. **Keyword Retrieval:** The cornerstone of traditional search, effective for straightforward queries but less so for complex or misspelled inputs.\n",
    "\n",
    "2. **Vector Retrieval:** A modern twist using embeddings to understand semantic similarities, overcoming the limitations of keyword-only searches.\n",
    "\n",
    "3. **Hybrid Retrieval:** A best-of-both-worlds strategy that combines keyword and vector methods to cover all bases, ensuring no query is left behind.\n",
    "\n",
    "4. **Semantic Ranking:** The final touch, reordering results to prioritize relevance, leveraging deep learning models inspired by Microsoft Bing's algorithms.\n",
    "\n",
    "#### üé¢ Strategy and Objectives: Navigating the Path to Excellence\n",
    "\n",
    "Our roadmap is clear: harness the phi-3 model to categorize user queries with razor-sharp accuracy, ensuring each query activates the most suitable retrieval mechanism. This strategy promises to significantly enhance system performance. Key milestones include:\n",
    "\n",
    "- **Crafting the Perfect Dataset:** A curated collection of data designed to boost the phi-3 model's query classification prowess.\n",
    "  \n",
    "- **Building a Superior Retrieval System:** A cutting-edge system that marries keyword and vector search methods with semantic ranking for unmatched relevance.\n",
    "  \n",
    "- **Setting the Performance Bar High:** Rigorous evaluation using leading metrics to ensure our system sets new standards for effectiveness.\n",
    "\n",
    "#### üöß From Blueprint to Reality: The Journey Ahead\n",
    "\n",
    "We're set to embark on a journey of fine-tuning and system integration, with the following phases:\n",
    "\n",
    "1. **Dataset Curation:** Assembling a rich dataset to train the phi-3 model, covering a wide spectrum of query types.\n",
    "   \n",
    "2. **Precision Fine-Tuning:** Employing QLoRA to fine-tune the phi-3 model, enhancing its ability to accurately classify and route queries.\n",
    "   \n",
    "3. **Seamless System Integration:** Incorporating the refined model into our retrieval framework, tailored to adapt to diverse search scenarios.\n",
    "   \n",
    "4. **Benchmarking Success:** Evaluating our system's performance in real-world and controlled environments to confirm our anticipated gains in accuracy, responsiveness, and efficiency.\n",
    "\n",
    "By embarking on this project, we aim to not only elevate the precision and efficiency of our retrieval system but also to redefine the benchmarks for language processing tasks through innovative fine-tuning of the phi-3 model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup Azure Machine Learning (AML) Requirements\n",
    "\n",
    "To establish a connection with an Azure Machine Learning workspace, specific identifying parameters are required: a subscription ID, a resource group name, and a workspace name. These details will be utilized with the `MLClient` from `azure.ai.ml` to access the desired Azure Machine Learning workspace. For authentication, the default Azure credentials will be employed in this hands-on guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "from azure.ai.ml import MLClient, Input\n",
    "from azure.ai.ml.dsl import pipeline\n",
    "from azure.ai.ml import load_component\n",
    "from azure.ai.ml import command\n",
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml import Input\n",
    "from azure.ai.ml import Output\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Gather values from .env\n",
    "AZURE_SUBSCRIPTION_ID = os.getenv('AZURE_SUBSCRIPTION_ID')\n",
    "AZURE_RESOURCE_GROUP = os.getenv('AZURE_RESOURCE_GROUP')\n",
    "AZURE_WORKSPACE = os.getenv('AZURE_WORKSPACE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open(r'src\\finetuning\\phi3\\config.yaml') as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "# Configuration for Azure and model setup\n",
    "azure_config = {\n",
    "    'data_directory': config['config']['data_directory'],\n",
    "    'cloud_storage_directory': config['config']['cloud_storage_directory'],\n",
    "    'model_identifier': config['config']['model_identifier'],\n",
    "    'debug_mode_enabled': config['config']['debug_mode_enabled'],\n",
    "    'use_low_priority_vm_option': config['config']['use_low_priority_vm_option']\n",
    "}\n",
    "\n",
    "# Training environment settings on Azure\n",
    "azure_training_settings = {\n",
    "    'azure_environment_name': config['training_settings']['azure_environment_name'],\n",
    "    'azure_compute_cluster_name': config['training_settings']['azure_compute_cluster_name'],\n",
    "    'azure_compute_cluster_vm_size': config['training_settings']['azure_compute_cluster_vm_size']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.aml_helper import AMLManager\n",
    "\n",
    "ml_manager = AMLManager(AZURE_SUBSCRIPTION_ID, \n",
    "                        AZURE_RESOURCE_GROUP, \n",
    "                        AZURE_WORKSPACE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-10 17:12:33,419 - micro - MainProcess - ERROR    Exception: Found Environment asset, but will update the Environment. (aml_helper.py:get_or_create_environment_asset:67)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\pablosal\\Desktop\\gbba-ai-small-language-models\\src\\aml_helper.py\", line 62, in get_or_create_environment_asset\n",
      "    raise ResourceExistsError('Found Environment asset, but will update the Environment.')\n",
      "azure.core.exceptions.ResourceExistsError: Found Environment asset, but will update the Environment.\n",
      "2024-07-10 17:12:34,635 - micro - MainProcess - INFO     Created Environment asset: llm-finetuning-phi3-2024-07-10. (aml_helper.py:get_or_create_environment_asset:75)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Environment({'arm_type': 'environment_version', 'latest_version': None, 'image': 'mcr.microsoft.com/azureml/curated/acft-hf-nlp-gpu:latest', 'intellectual_property': None, 'is_anonymous': False, 'auto_increment_version': False, 'auto_delete_setting': None, 'name': 'llm-finetuning-phi3-2024-07-10', 'description': 'Environment created for llm fine-tuning.', 'tags': {}, 'properties': {'azureml.labels': 'latest'}, 'print_as_yaml': False, 'id': '/subscriptions/1a4bb722-f155-4502-8033-022a9eb1481b/resourceGroups/dev/providers/Microsoft.MachineLearningServices/workspaces/ml-workspace-dev-eastus-001/environments/llm-finetuning-phi3-2024-07-10/versions/2', 'Resource__source_path': '', 'base_path': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbba-ai-small-language-models', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x000001D1FB358C40>, 'serialize': <msrest.serialization.Serializer object at 0x000001D1FB358A30>, 'version': '2', 'conda_file': {'channels': ['conda-forge'], 'dependencies': ['python=3.8', 'pip=24.0', {'pip': ['bitsandbytes==0.43.1', 'transformers~=4.41', 'peft~=0.11', 'accelerate~=0.30', 'trl==0.8.6', 'einops==0.8.0', 'datasets==2.19.1', 'wandb==0.17.0', 'mlflow==2.13.0', 'azureml-mlflow==1.56.0', 'torchvision==0.18.0']}], 'name': 'model-env'}, 'build': None, 'inference_config': None, 'os_type': 'Linux', 'conda_file_path': None, 'path': None, 'datastore': None, 'upload_hash': None, 'translated_conda_file': '{\\n  \"channels\": [\\n    \"conda-forge\"\\n  ],\\n  \"dependencies\": [\\n    \"python=3.8\",\\n    \"pip=24.0\",\\n    {\\n      \"pip\": [\\n        \"bitsandbytes==0.43.1\",\\n        \"transformers~=4.41\",\\n        \"peft~=0.11\",\\n        \"accelerate~=0.30\",\\n        \"trl==0.8.6\",\\n        \"einops==0.8.0\",\\n        \"datasets==2.19.1\",\\n        \"wandb==0.17.0\",\\n        \"mlflow==2.13.0\",\\n        \"azureml-mlflow==1.56.0\",\\n        \"torchvision==0.18.0\"\\n      ]\\n    }\\n  ],\\n  \"name\": \"model-env\"\\n}'})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_manager.get_or_create_environment_asset(azure_training_settings['azure_environment_name'],\n",
    "                                           conda_yml=\"src/finetuning/phi3/env/conda.yaml\",\n",
    "                                           update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_manager.get_or_create_environment_asset(azure_training_settings['azure_environment_name'],\n",
    "                                           conda_yml=\"src/finetuning/phi3/env/conda.yaml\",\n",
    "                                           update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-10 17:15:02,402 - micro - MainProcess - INFO     Compute cluster 'gpu-cluster-phi3' does not exist. Creating a new one with size 'Standard_NC12s_v3', tier 'lowpriority', and max instances '1'. (aml_helper.py:create_or_reuse_compute_cluster:129)\n",
      "2024-07-10 17:15:34,055 - micro - MainProcess - INFO     Compute cluster 'gpu-cluster-phi3' created successfully. (aml_helper.py:create_or_reuse_compute_cluster:139)\n"
     ]
    }
   ],
   "source": [
    "# Create or reuse the compute cluster\n",
    "compute = ml_manager.create_or_reuse_compute_cluster(\n",
    "    azure_training_settings['azure_compute_cluster_name'], \n",
    "    azure_training_settings['azure_compute_cluster_vm_size'],\n",
    "    tier='lowpriority',\n",
    "    max_instances=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Curation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section outlines the approach for generating training and evaluation datasets, specifically tailored for query categorization and retrieval tasks. The process leverages the \"Contoso Employee Handbook\" as the source document, ensuring relevance and consistency across query categories.\n",
    "\n",
    "#### Step 1: Categorization of Queries\n",
    "To accommodate diverse query types, we've delineated the following categories:\n",
    "\n",
    "- **Concept Seeking Queries**: Abstract inquiries necessitating elaborate responses.\n",
    "- **Exact Snippet Search**: Precise, lengthy queries directly extracted from the source text.\n",
    "- **Web Search-like Queries**: Concise queries mimicking typical search engine inputs.\n",
    "- **Low Query/Doc Term Overlap**: Queries and answers with minimal lexical similarity.\n",
    "- **Fact Seeking Queries**: Direct questions expecting singular, definitive answers.\n",
    "- **Keyword Queries**: Brief, focused queries emphasizing critical terms.\n",
    "- **Queries with Misspellings**: Queries incorporating common spelling errors.\n",
    "- **Long Queries**: Inquiries exceeding 20 tokens in length.\n",
    "- **Medium Queries**: Queries spanning between 5 to 20 tokens.\n",
    "- **Short Queries**: Queries comprising fewer than 5 tokens.\n",
    "\n",
    "**Source Document**\n",
    "The foundational text for this exercise is the \"Contoso Employee Handbook,\" a comprehensive guide detailing the policies and procedures for Contoso employees.\n",
    "\n",
    "#### Step 2: Training Dataset Generation\n",
    "For each query category, using gpt4 I crafted 20 distinct questions derived from the \"Contoso Employee Handbook,\" ensuring a broad representation of potential inquiries. Each question was meticulously categorized to reflect its nature accurately.\n",
    "\n",
    "\n",
    "#### Step 3: Evaluation Dataset Creation\n",
    "To assess the model's performance, I formulated again using gpt4 5 unique questions per category, distinct from the training set yet pertinent to the \"Contoso Employee Handbook.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Kind of Query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the purpose of the Contoso Employee Ha...</td>\n",
       "      <td>Concept Seeking queries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How does Contoso define at-will employment?</td>\n",
       "      <td>Concept Seeking queries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the classifications of employees at C...</td>\n",
       "      <td>Concept Seeking queries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Explain Contoso's policy on equal employment o...</td>\n",
       "      <td>Concept Seeking queries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What does Contoso's confidentiality policy ent...</td>\n",
       "      <td>Concept Seeking queries</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question            Kind of Query\n",
       "0  What is the purpose of the Contoso Employee Ha...  Concept Seeking queries\n",
       "1        How does Contoso define at-will employment?  Concept Seeking queries\n",
       "2  What are the classifications of employees at C...  Concept Seeking queries\n",
       "3  Explain Contoso's policy on equal employment o...  Concept Seeking queries\n",
       "4  What does Contoso's confidentiality policy ent...  Concept Seeking queries"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_train = pd.read_csv(r\"src\\finetuning\\phi3\\data\\retrieval_train.csv\")\n",
    "df_val = pd.read_csv(r\"src\\finetuning\\phi3\\data\\retrieval_train.csv\")\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating training script "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/finetuning/phi3/train_mlflow.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/finetuning/phi3/train_mlflow.py\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.types import DataType, Schema, ColSpec\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "\n",
    "import os\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "import argparse\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "import torch\n",
    "import transformers\n",
    "from trl import SFTTrainer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "from datetime import datetime\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_model(model_name_or_path: str = \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "               use_cache: bool = False,\n",
    "               trust_remote_code: bool = True,\n",
    "               torch_dtype: torch.dtype = torch.bfloat16,\n",
    "               device_map: dict = None,\n",
    "               max_seq_length: int = 4096) -> tuple:\n",
    "    \"\"\"\n",
    "    Loads a pre-trained model and its tokenizer with specified configurations.\n",
    "\n",
    "    Parameters:\n",
    "    - model_name_or_path (str): Identifier for the model to load. Can be a model ID or path.\n",
    "    - use_cache (bool): Whether to use caching for model outputs.\n",
    "    - trust_remote_code (bool): Whether to trust remote code when loading the model.\n",
    "    - torch_dtype (torch.dtype): Data type for model tensors. Recommended to use torch.bfloat16 for efficiency.\n",
    "    - device_map (dict): Custom device map for distributing the model's layers across devices.\n",
    "    - max_seq_length (int): Maximum sequence length for the tokenizer.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple containing the loaded model and tokenizer.\n",
    "    \"\"\"\n",
    "    model_kwargs = {\n",
    "        \"use_cache\": use_cache,\n",
    "        \"trust_remote_code\": trust_remote_code,\n",
    "        \"torch_dtype\": torch_dtype,\n",
    "        \"device_map\": device_map\n",
    "    }\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name_or_path, **model_kwargs)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "    tokenizer.model_max_length = max_seq_length\n",
    "    tokenizer.pad_token = tokenizer.unk_token  # use unk rather than eos token to prevent endless generation\n",
    "    tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    return model, tokenizer\n",
    "\n",
    "def convert_to_chat_format(df: pd.DataFrame) -> List[List[Dict[str, str]]]:\n",
    "    \"\"\"\n",
    "    Converts a DataFrame containing questions and their types into a chat format.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): A DataFrame with at least two columns: 'Question' and 'Kind of Query'.\n",
    "      Each row represents a user query and its categorization.\n",
    "\n",
    "    Returns:\n",
    "    - List[List[Dict[str, str]]]: A list of chats, where each chat is a list of messages.\n",
    "      Each message is a dictionary with 'role' and 'content' keys. The 'role' can be 'system',\n",
    "      'user', or 'assistant', indicating the sender of the message. The 'content' is the text of the message.\n",
    "    \"\"\"\n",
    "    chats = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        chat = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an AI assistant supporting users by categorizing their queries.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": row[\"Question\"]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": f\"This query is a '{row['Kind of Query']}' type.\"\n",
    "            }\n",
    "        ]\n",
    "        chats.append(chat)\n",
    "    \n",
    "    return chats\n",
    "\n",
    "def convert_chats_to_dataframe(chats: List[List[Dict[str, str]]]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Converts a list of chats into a DataFrame where each chat is represented as a dictionary in the 'message' column.\n",
    "\n",
    "    Parameters:\n",
    "    - chats (List[List[Dict[str, str]]]): A list of chats, where each chat is a list of messages.\n",
    "      Each message is a dictionary with 'role' and 'content' keys.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame with a single column 'message', where each row contains a dictionary\n",
    "      representing a chat.\n",
    "    \"\"\"\n",
    "    # Convert each chat into a dictionary and store it in a list\n",
    "    chat_dicts = [{'message': chat} for chat in chats]\n",
    "    \n",
    "    # Create a DataFrame from the list of dictionaries\n",
    "    df = pd.DataFrame(chat_dicts)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def apply_chat_template(\n",
    "    example: dict,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Applies a chat template to the messages in an example from a dataset.\n",
    "\n",
    "    This function modifies the input example by adding a system message at the beginning\n",
    "    if it does not already start with one. It then applies a chat template formatting\n",
    "    using the specified tokenizer.\n",
    "\n",
    "    Parameters:\n",
    "    - example (dict): A dictionary representing a single example from a dataset. It must\n",
    "      contain a key 'messages', which is a list of message dictionaries. Each message\n",
    "      dictionary should have 'role' and 'content' keys.\n",
    "    - tokenizer (PreTrainedTokenizer): An instance of a tokenizer that supports the\n",
    "      `apply_chat_template` method for formatting chat messages.\n",
    "\n",
    "    Returns:\n",
    "    - dict: The modified example dictionary with an added 'text' key that contains the\n",
    "      formatted chat as a string.\n",
    "    \"\"\"\n",
    "    messages = example[\"message\"]\n",
    "    # Add an empty system message if there is none\n",
    "    if messages[0][\"role\"] != \"system\":\n",
    "        messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n",
    "    example[\"text\"] = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=False)\n",
    "    return example\n",
    "\n",
    "def main(args):\n",
    "    \n",
    "    ###################\n",
    "    # Hyper-parameters\n",
    "    ###################\n",
    "    # Only overwrite environ if wandb param passed\n",
    "    if len(args.wandb_project) > 0:\n",
    "        os.environ['WANDB_API_KEY'] = args.wandb_api_key    \n",
    "        os.environ[\"WANDB_PROJECT\"] = args.wandb_project\n",
    "    if len(args.wandb_watch) > 0:\n",
    "        os.environ[\"WANDB_WATCH\"] = args.wandb_watch\n",
    "    if len(args.wandb_log_model) > 0:\n",
    "        os.environ[\"WANDB_LOG_MODEL\"] = args.wandb_log_model\n",
    "\n",
    "    use_wandb = len(args.wandb_project) > 0 or (\"WANDB_PROJECT\" in os.environ and len(os.environ[\"WANDB_PROJECT\"]) > 0) \n",
    "\n",
    "    training_config = {\n",
    "        \"bf16\": True,\n",
    "        \"do_eval\": False,\n",
    "        \"learning_rate\": args.learning_rate,\n",
    "        \"log_level\": \"info\",\n",
    "        \"logging_steps\": args.logging_steps,\n",
    "        \"logging_strategy\": \"steps\",\n",
    "        \"lr_scheduler_type\": args.lr_scheduler_type,\n",
    "        \"num_train_epochs\": args.epochs,\n",
    "        \"max_steps\": -1,\n",
    "        \"output_dir\": args.output_dir,\n",
    "        \"overwrite_output_dir\": True,\n",
    "        \"per_device_train_batch_size\": args.train_batch_size,\n",
    "        \"per_device_eval_batch_size\": args.eval_batch_size,\n",
    "        \"remove_unused_columns\": True,\n",
    "        \"save_steps\": args.save_steps,\n",
    "        \"save_total_limit\": 1,\n",
    "        \"seed\": args.seed,\n",
    "        \"gradient_checkpointing\": True,\n",
    "        \"gradient_checkpointing_kwargs\": {\"use_reentrant\": False},\n",
    "        \"gradient_accumulation_steps\": args.grad_accum_steps,\n",
    "        \"warmup_ratio\": args.warmup_ratio,\n",
    "    }\n",
    "\n",
    "    peft_config = {\n",
    "        \"r\": args.lora_r,\n",
    "        \"lora_alpha\": args.lora_alpha,\n",
    "        \"lora_dropout\": args.lora_dropout,\n",
    "        \"bias\": \"none\",\n",
    "        \"task_type\": \"CAUSAL_LM\",\n",
    "        #\"target_modules\": \"all-linear\",\n",
    "        \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        \"modules_to_save\": None,\n",
    "    }\n",
    "\n",
    "    checkpoint_dir = os.path.join(args.output_dir, \"checkpoints\")\n",
    "\n",
    "    train_conf = TrainingArguments(\n",
    "        **training_config,\n",
    "        report_to=\"wandb\" if use_wandb else \"azure_ml\",\n",
    "        run_name=args.wandb_run_name if use_wandb else None,    \n",
    "    )\n",
    "    peft_conf = LoraConfig(**peft_config)\n",
    "    model, tokenizer = load_model(args)\n",
    "\n",
    "    ###############\n",
    "    # Setup logging\n",
    "    ###############\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "        handlers=[logging.StreamHandler(sys.stdout)],\n",
    "    )\n",
    "    log_level = train_conf.get_process_log_level()\n",
    "    logger.setLevel(log_level)\n",
    "    datasets.utils.logging.set_verbosity(log_level)\n",
    "    transformers.utils.logging.set_verbosity(log_level)\n",
    "    transformers.utils.logging.enable_default_handler()\n",
    "    transformers.utils.logging.enable_explicit_format()\n",
    "\n",
    "    # Log on each process a small summary\n",
    "    logger.warning(\n",
    "        f\"Process rank: {train_conf.local_rank}, device: {train_conf.device}, n_gpu: {train_conf.n_gpu}\"\n",
    "        + f\" distributed training: {bool(train_conf.local_rank != -1)}, 16-bits training: {train_conf.fp16}\"\n",
    "    )\n",
    "    logger.info(f\"Training/evaluation parameters {train_conf}\")\n",
    "    logger.info(f\"PEFT parameters {peft_conf}\")    \n",
    "\n",
    "    ##################\n",
    "    # Data Processing\n",
    "    ##################\n",
    "\n",
    "    train_dataset = load_dataset('json', data_files=os.path.join(args.train_dir, 'train.jsonl'), split='train')\n",
    "    eval_dataset = load_dataset('json', data_files=os.path.join(args.train_dir, 'eval.jsonl'), split='train')\n",
    "    column_names = list(train_dataset.features)\n",
    "\n",
    "    train_data = pd.read_csv(args.train_dir)\n",
    "    train_data_chat_format = convert_to_chat_format(train_data)\n",
    "    df_train_data_chat_format = convert_chats_to_dataframe(train_data_chat_format)\n",
    "    train_dataset = datasets.Dataset.from_pandas(pd.DataFrame(df_train_data_chat_format, columns=[\"message\"]),split= \"train\")\n",
    "\n",
    "    eval_data = pd.read_csv(args.eval_data)\n",
    "    eval_data_chat_format = convert_to_chat_format(eval_data)\n",
    "    df_eval_data_chat_format = convert_chats_to_dataframe(eval_data_chat_format)\n",
    "    eval_data_dataset = datasets.Dataset.from_pandas(pd.DataFrame(df_eval_data_chat_format, columns=[\"message\"]),split=\"train\")\n",
    "\n",
    "    column_names = list(train_dataset.features)\n",
    "\n",
    "    processed_train_dataset = train_dataset.map(\n",
    "        apply_chat_template,\n",
    "        fn_kwargs={\"tokenizer\": tokenizer},\n",
    "        num_proc=10,\n",
    "        remove_columns=column_names,\n",
    "        desc=\"Applying chat template to train_sft\",\n",
    "    )\n",
    "\n",
    "    processed_eval_dataset = eval_dataset.map(\n",
    "        apply_chat_template,\n",
    "        fn_kwargs={\"tokenizer\": tokenizer},\n",
    "        num_proc=10,\n",
    "        remove_columns=column_names,\n",
    "        desc=\"Applying chat template to test_sft\",\n",
    "    )\n",
    "\n",
    "    with mlflow.start_run() as run:        \n",
    "        ###########\n",
    "        # Training\n",
    "        ###########\n",
    "        trainer = SFTTrainer(\n",
    "            model=model,\n",
    "            args=train_conf,\n",
    "            peft_config=peft_conf,\n",
    "            train_dataset=processed_train_dataset,\n",
    "            eval_dataset=processed_eval_dataset,\n",
    "            max_seq_length=args.max_seq_length,\n",
    "            dataset_text_field=\"text\",\n",
    "            tokenizer=tokenizer,\n",
    "            packing=True,\n",
    "        )\n",
    "\n",
    "        # Show current memory stats\n",
    "        gpu_stats = torch.cuda.get_device_properties(0)\n",
    "        start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "        max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "        logger.info(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "        logger.info(f\"{start_gpu_memory} GB of memory reserved.\")\n",
    "        \n",
    "        last_checkpoint = None\n",
    "        if os.path.isdir(checkpoint_dir):\n",
    "            checkpoints = [os.path.join(checkpoint_dir, d) for d in os.listdir(checkpoint_dir)]\n",
    "            if len(checkpoints) > 0:\n",
    "                checkpoints.sort(key=os.path.getmtime, reverse=True)\n",
    "                last_checkpoint = checkpoints[0]        \n",
    "\n",
    "        trainer_stats = trainer.train(resume_from_checkpoint=last_checkpoint)\n",
    "\n",
    "        #############\n",
    "        # Logging\n",
    "        #############\n",
    "        metrics = trainer_stats.metrics\n",
    "\n",
    "        # Show final memory and time stats \n",
    "        used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "        used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "        used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "        lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "\n",
    "        logger.info(f\"{metrics['train_runtime']} seconds used for training.\")\n",
    "        logger.info(f\"{round(metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "        logger.info(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "        logger.info(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "        logger.info(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "        logger.info(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")\n",
    "                \n",
    "        trainer.log_metrics(\"train\", metrics)\n",
    "\n",
    "        model_info = mlflow.transformers.log_model(\n",
    "            transformers_model={\"model\": trainer.model, \"tokenizer\": tokenizer},\n",
    "            #prompt_template=prompt_template,\n",
    "            #signature=signature,\n",
    "            artifact_path=args.model_dir,  # This is a relative path to save model files within MLflow run\n",
    "        )\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    # setup argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # curr_time = datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "\n",
    "    # hyperparameters\n",
    "    parser.add_argument(\"--train_dir\", default=\"data\", type=str, help=\"Input directory for training\")\n",
    "    parser.add_argument(\"--model_dir\", default=\"./model\", type=str, help=\"output directory for model\")\n",
    "    parser.add_argument(\"--epochs\", default=1, type=int, help=\"number of epochs\")\n",
    "    parser.add_argument(\"--output_dir\", default=\"./output_dir\", type=str, help=\"directory to temporarily store when training a model\")\n",
    "    parser.add_argument(\"--train_batch_size\", default=8, type=int, help=\"training - mini batch size for each gpu/process\")\n",
    "    parser.add_argument(\"--eval_batch_size\", default=8, type=int, help=\"evaluation - mini batch size for each gpu/process\")\n",
    "    parser.add_argument(\"--learning_rate\", default=5e-06, type=float, help=\"learning rate\")\n",
    "    parser.add_argument(\"--logging_steps\", default=2, type=int, help=\"logging steps\")\n",
    "    parser.add_argument(\"--save_steps\", default=100, type=int, help=\"save steps\")    \n",
    "    parser.add_argument(\"--grad_accum_steps\", default=4, type=int, help=\"gradient accumulation steps\")\n",
    "    parser.add_argument(\"--lr_scheduler_type\", default=\"linear\", type=str)\n",
    "    parser.add_argument(\"--seed\", default=0, type=int, help=\"seed\")\n",
    "    parser.add_argument(\"--warmup_ratio\", default=0.2, type=float, help=\"warmup ratio\")\n",
    "    parser.add_argument(\"--max_seq_length\", default=2048, type=int, help=\"max seq length\")\n",
    "    parser.add_argument(\"--save_merged_model\", type=bool, default=False)\n",
    "\n",
    "    # lora hyperparameters\n",
    "    parser.add_argument(\"--lora_r\", default=16, type=int, help=\"lora r\")\n",
    "    parser.add_argument(\"--lora_alpha\", default=16, type=int, help=\"lora alpha\")\n",
    "    parser.add_argument(\"--lora_dropout\", default=0.05, type=float, help=\"lora dropout\")\n",
    "    \n",
    "    # wandb params\n",
    "    parser.add_argument(\"--wandb_api_key\", type=str, default=\"\")\n",
    "    parser.add_argument(\"--wandb_project\", type=str, default=\"\")\n",
    "    parser.add_argument(\"--wandb_run_name\", type=str, default=\"\")\n",
    "    parser.add_argument(\"--wandb_watch\", type=str, default=\"gradients\") # options: false | gradients | all\n",
    "    parser.add_argument(\"--wandb_log_model\", type=str, default=\"false\") # options: false | true\n",
    "\n",
    "    # parse args\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # return args\n",
    "    return args\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #sys.argv = ['']\n",
    "    args = parse_args()\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import command\n",
    "from azure.ai.ml import Input\n",
    "from azure.ai.ml.entities import ResourceConfiguration\n",
    "\n",
    "job = command(\n",
    "    inputs=dict(\n",
    "        #train_dir=Input(type=\"uri_folder\", path=DATA_DIR), # Get data from local path\n",
    "        train_dir=Input(path=f\"{AZURE_DATA_NAME}@latest\"),  # Get data from Data asset\n",
    "        epoch=d['train']['epoch'],\n",
    "        train_batch_size=d['train']['train_batch_size'],\n",
    "        eval_batch_size=d['train']['eval_batch_size'],  \n",
    "        model_dir=d['train']['model_dir']\n",
    "    ),\n",
    "    code=\"./src_train\",  # local path where the code is stored\n",
    "    compute=azure_compute_cluster_name,\n",
    "    command=\"python train_mlflow.py --train_dir ${{inputs.train_dir}} --epochs ${{inputs.epoch}} --train_batch_size ${{inputs.train_batch_size}} --eval_batch_size ${{inputs.eval_batch_size}} --model_dir ${{inputs.model_dir}}\",\n",
    "    #environment=\"azureml://registries/azureml/environments/acft-hf-nlp-gpu/versions/57\", # Use built-in Environment asset\n",
    "    environment=f\"{azure_env_name}@latest\",\n",
    "    distribution={\n",
    "        \"type\": \"PyTorch\",\n",
    "        \"process_count_per_instance\": 1, # For multi-gpu training set this to an integer value more than 1\n",
    "    },\n",
    ")\n",
    "returned_job = ml_client.jobs.create_or_update(job)\n",
    "ml_client.jobs.stream(returned_job.name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vector-indexing-azureaisearch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
